{
    "summary": "The code trains a logistic regression model using weak and strong labels, evaluates accuracy, and compares performance; it also imports libraries, defines functions, and includes features such as data loading, embedding checking, and epoch-based updating.",
    "details": [
        {
            "comment": "This code is importing necessary libraries, defining a function to obtain model embeddings from a given loader. It also contains a function to select the desired model for the task.\nThe code allows users to get model embeddings by providing a specific name for each predefined model (alexnet, resnet50_dino, and vitb8_dino). The model is then put in evaluation mode, and all models are parallelized using nn.DataParallel() for speedup.\nThe embeddings obtained from the model are saved into lists for further processing. The code uses tqdm for progress bar display during the execution process.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":0-40",
            "content": "import fire\nimport numpy as np\nimport torch\nimport tqdm\nfrom data import get_imagenet\nfrom models import alexnet, resnet50_dino, vitb8_dino\nfrom torch import nn\ndef get_model(name):\n    if name == \"alexnet\":\n        model = alexnet()\n    elif name == \"resnet50_dino\":\n        model = resnet50_dino()\n    elif name == \"vitb8_dino\":\n        model = vitb8_dino()\n    else:\n        raise ValueError(f\"Unknown model {name}\")\n    model.cuda()\n    model.eval()\n    model = nn.DataParallel(model)\n    return model\ndef get_embeddings(model, loader):\n    all_embeddings, all_y, all_probs = [], [], []\n    for x, y in tqdm.tqdm(loader):\n        output = model(x.cuda())\n        if len(output) == 2:\n            embeddings, logits = output\n            probs = torch.nn.functional.softmax(logits, dim=-1).detach().cpu()\n            all_probs.append(probs)\n        else:\n            embeddings = output\n        all_embeddings.append(embeddings.detach().cpu())\n        all_y.append(y)\n    all_embeddings = torch.cat(all_embeddings, axis=0)\n    all_y = torch.cat(all_y, axis=0)"
        },
        {
            "comment": "The code defines a function `train_logreg` that takes in training and evaluation datasets, number of epochs, weight decay, learning rate, batch size, and number of classes. It performs logistic regression training using the Adam optimizer with cosine annealing learning rate scheduling. The function returns the all_embeddings, all_y, all_probs, and accuracy for the evaluation datasets.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":41-72",
            "content": "    if len(all_probs) > 0:\n        all_probs = torch.cat(all_probs, axis=0)\n        acc = (torch.argmax(all_probs, dim=1) == all_y).float().mean()\n    else:\n        all_probs = None\n        acc = None\n    return all_embeddings, all_y, all_probs, acc\ndef train_logreg(\n    x_train,\n    y_train,\n    eval_datasets,\n    n_epochs=10,\n    weight_decay=0.0,\n    lr=1.0e-3,\n    batch_size=100,\n    n_classes=1000,\n):\n    x_train = x_train.float()\n    train_ds = torch.utils.data.TensorDataset(x_train, y_train)\n    train_loader = torch.utils.data.DataLoader(train_ds, shuffle=True, batch_size=batch_size)\n    d = x_train.shape[1]\n    model = torch.nn.Linear(d, n_classes).cuda()\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), weight_decay=weight_decay, lr=lr)\n    n_batches = len(train_loader)\n    n_iter = n_batches * n_epochs\n    schedule = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=n_iter)\n    results = {f\"{key}_all\": [] for key in eval_datasets.keys()}"
        },
        {
            "comment": "Looping over epochs, updating the model using training data, and calculating accuracy on validation sets.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":73-101",
            "content": "    for epoch in (pbar := tqdm.tqdm(range(n_epochs), desc=\"Epoch 0\")):\n        correct, total = 0, 0\n        for x, y in train_loader:\n            x, y = x.cuda(), y.cuda()\n            optimizer.zero_grad()\n            pred = model(x)\n            loss = criterion(pred, y)\n            loss.backward()\n            optimizer.step()\n            schedule.step()\n            if len(y.shape) > 1:\n                y = torch.argmax(y, dim=1)\n            correct += (torch.argmax(pred, -1) == y).detach().float().sum().item()\n            total += len(y)\n        pbar.set_description(f\"Epoch {epoch}, Train Acc {correct / total:.3f}\")\n        for key, (x_test, y_test) in eval_datasets.items():\n            x_test = x_test.float().cuda()\n            pred = torch.argmax(model(x_test), axis=-1).detach().cpu()\n            acc = (pred == y_test).float().mean()\n            results[f\"{key}_all\"].append(acc)\n    for key in eval_datasets.keys():\n        results[key] = results[f\"{key}_all\"][-1]\n    return results\ndef main(\n    batch_size: int = 128,"
        },
        {
            "comment": "The code initializes weak and strong models, loads data, gets weak labels with the weak model, checks for consistency in strong embeddings, shuffles the order of embeddings, splits data into training and testing sets.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":102-129",
            "content": "    weak_model_name: str = \"alexnet\",\n    strong_model_name: str = \"resnet50_dino\",\n    n_train: int = 40000,\n    seed: int = 0,\n    data_path: str = \"/root/\",\n    n_epochs: int = 10,\n    lr: float = 1e-3,\n):\n    weak_model = get_model(weak_model_name)\n    strong_model = get_model(strong_model_name)\n    _, loader = get_imagenet(data_path, split=\"val\", batch_size=batch_size, shuffle=False)\n    print(\"Getting weak labels...\")\n    _, gt_labels, weak_labels, weak_acc = get_embeddings(weak_model, loader)\n    print(f\"Weak label accuracy: {weak_acc:.3f}\")\n    print(\"Getting strong embeddings...\")\n    embeddings, strong_gt_labels, _, _ = get_embeddings(strong_model, loader)\n    assert torch.all(gt_labels == strong_gt_labels)\n    del strong_gt_labels\n    order = np.arange(len(embeddings))\n    rng = np.random.default_rng(seed)\n    rng.shuffle(order)\n    x = embeddings[order]\n    y = gt_labels[order]\n    yw = weak_labels[order]\n    x_train, x_test = x[:n_train], x[n_train:]\n    y_train, y_test = y[:n_train], y[n_train:]\n    yw_train, yw_test = yw[:n_train], yw[n_train:]"
        },
        {
            "comment": "This code is training a logistic regression model on weak and strong labels for classification tasks. It first calculates the argmax of yw_test, creates an evaluation dataset with weak labels, trains the logreg model using train_logreg function, prints the final accuracy and supervisor-student agreement, plots accuracy by epoch, and then trains a logreg model on ground truth labels for comparison. Finally, it displays the weak label accuracy and weak to strong accuracy.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":130-149",
            "content": "    yw_test = torch.argmax(yw_test, dim=1)\n    eval_datasets = {\"test\": (x_test, y_test), \"test_weak\": (x_test, yw_test)}\n    print(\"Training logreg on weak labels...\")\n    results_weak = train_logreg(x_train, yw_train, eval_datasets, n_epochs=n_epochs, lr=lr)\n    print(f\"Final accuracy: {results_weak['test']:.3f}\")\n    print(f\"Final supervisor-student agreement: {results_weak['test_weak']:.3f}\")\n    print(f\"Accuracy by epoch: {[acc.item() for acc in results_weak['test_all']]}\")\n    print(\n        f\"Supervisor-student agreement by epoch: {[acc.item() for acc in results_weak['test_weak_all']]}\"\n    )\n    print(\"Training logreg on ground truth labels...\")\n    results_gt = train_logreg(x_train, y_train, eval_datasets, n_epochs=n_epochs, lr=lr)\n    print(f\"Final accuracy: {results_gt['test']:.3f}\")\n    print(f\"Accuracy by epoch: {[acc.item() for acc in results_gt['test_all']]}\")\n    print(\"\\n\\n\" + \"=\" * 100)\n    print(f\"Weak label accuracy: {weak_acc:.3f}\")\n    print(f\"Weak\u2192Strong accuracy: {results_weak['test']:.3f}\")"
        },
        {
            "comment": "Printing strong accuracy value and separating with a line of 100 equals signs.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/vision/run_weak_strong.py\":150-155",
            "content": "    print(f\"Strong accuracy: {results_gt['test']:.3f}\")\n    print(\"=\" * 100)\nif __name__ == \"__main__\":\n    fire.Fire(main)"
        }
    ]
}