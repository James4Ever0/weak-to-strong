{
    "summary": "The code imports necessary libraries, defines configurations for precision and utilization, loads datasets, trains models using transfer learning, compares performance, calculates improvements, stores results, and summarizes findings in a JSON file.",
    "details": [
        {
            "comment": "This code imports necessary libraries, defines a list of ModelConfigs with default learning rates and batch sizes, and includes functions for dataset loading and model training. It also checks if BF16 or FP32 is supported on the GPU.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":0-32",
            "content": "import json\nimport os\nfrom typing import Dict, List, Optional, Sequence, Union\nimport fire\nimport numpy as np\nimport torch\nimport weak_to_strong.logger as logger\nfrom weak_to_strong.common import get_tokenizer\nfrom weak_to_strong.datasets import (VALID_DATASETS, load_dataset,\n                                     tokenize_dataset)\nfrom weak_to_strong.loss import logconf_loss_fn, product_loss_fn, xent_loss\nfrom weak_to_strong.train import ModelConfig, train_and_save_model\n# NOTE learning rates are not particularly tuned, work somewhat reasonably at train batch size 32\nMODEL_CONFIGS = [\n    ModelConfig(\n        name=\"gpt2\",\n        default_lr=5e-5,\n        eval_batch_size=32,\n        custom_kwargs={\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"gpt2-medium\",\n        default_lr=5e-5,\n        eval_batch_size=32,\n        custom_kwargs={\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),"
        },
        {
            "comment": "Code contains a list of model configurations for various language models. Each configuration has properties like name, default learning rate, evaluation batch size, and custom kwargs (like bf16 and fp32 support). Some models also have additional properties like gradient checkpointing, model parallelism, and trust remote code.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":33-71",
            "content": "        },\n    ),\n    ModelConfig(\n        name=\"gpt2-large\",\n        default_lr=1e-5,\n        eval_batch_size=32,\n        custom_kwargs={\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"gpt2-xl\",\n        default_lr=1e-5,\n        eval_batch_size=2,\n        gradient_checkpointing=True,\n        model_parallel=True,\n        custom_kwargs={\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"Qwen/Qwen-1_8B\",\n        default_lr=1e-5,\n        eval_batch_size=2,\n        gradient_checkpointing=True,\n        model_parallel=True,\n        custom_kwargs={\n            \"trust_remote_code\": True,\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"Qwen/Qwen-7B\",\n        default_lr=1e-5,\n        eval_batch_size=2,\n        gradient_checkpointing=True,"
        },
        {
            "comment": "Defining multiple model configurations with different names, default learning rates, evaluation batch sizes, gradient checkpointing, and model parallelism. Requires bf16 support and many GPUs for execution.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":72-101",
            "content": "        model_parallel=True,\n        # note: you will probably not be able to run this without many gpus\n        custom_kwargs={\n            \"trust_remote_code\": True,\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"Qwen/Qwen-14B\",\n        default_lr=1e-5,\n        eval_batch_size=2,\n        gradient_checkpointing=True,\n        model_parallel=True,\n        # note: you will probably not be able to run this without bf16 support and many gpus\n        custom_kwargs={\n            \"trust_remote_code\": True,\n            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n    ),\n    ModelConfig(\n        name=\"Qwen/Qwen-72B\",\n        default_lr=1e-5,\n        eval_batch_size=1,\n        gradient_checkpointing=True,\n        model_parallel=True,\n        # note: you will probably not be able to run this without bf16 support and many gpus\n        custom_kwargs={\n            \"trust_remote_code\": True,"
        },
        {
            "comment": "This code defines a main function with several arguments and configures model settings for transfer learning. It uses different types of losses, such as logconf, product, and xent. The model configurations are stored in the MODELS_DICT dictionary, and default optimizers are set to adafactor. The code also checks if BF16 is supported by the CUDA and sets the precision accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":102-137",
            "content": "            \"bf16\": torch.cuda.is_bf16_supported(),\n            \"fp32\": not torch.cuda.is_bf16_supported(),\n        },\n        # This model is really big, save space by using adafactor.\n        # Note that even then it will take up ~60GB per GPU on an 8-GPU machine.\n        default_optimizer=\"adafactor\",\n    ),\n]\nMODELS_DICT: Dict[str, ModelConfig] = {\n    model_config.name: model_config for model_config in MODEL_CONFIGS\n}\nloss_dict = {\n    \"logconf\": logconf_loss_fn(),\n    \"product\": product_loss_fn(),\n    \"xent\": xent_loss(),\n}\nVALID_LOSSES: List[str] = list(loss_dict.keys())\ndef main(\n    batch_size: int = 32,\n    max_ctx: int = 1024,\n    ds_name: str = \"sciq\",\n    transfer_loss: Union[str, Sequence[str]] = \"xent,logconf\",\n    n_docs: int = 10000,\n    n_test_docs: int = 200,\n    weak_model_size: str = \"gpt2\",\n    weak_lr: Optional[float] = None,\n    strong_model_size: str = \"gpt2-xl\",\n    strong_lr: Optional[float] = None,\n    # Defaults to strong_lr\n    transfer_lr: Optional[float] = None,\n    # Optims default to default_optimizer in the model definitions"
        },
        {
            "comment": "The code defines various optional parameters for training a weak model to a strong one and transferring the knowledge. It sets default values, asserts that the specified dataset is valid, splits transfer loss if it's provided as a string, and checks that each transfer loss is also valid.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":138-165",
            "content": "    weak_optim: Optional[str] = None,\n    strong_optim: Optional[str] = None,\n    transfer_optim: Optional[str] = None,\n    gt_epochs: int = 2,\n    # defaults to gt_epochs\n    transfer_epochs: Optional[int] = None,\n    force_retrain: bool = False,\n    seed: int = 0,\n    minibatch_size_per_device: Optional[int] = None,\n    train_with_dropout: bool = False,\n    results_folder: str = \"/tmp/results\",\n    linear_probe: bool = False,\n    lr_schedule: str = \"cosine_anneal\",\n    log_prefix: str = \"\",\n    # Set to an absurdly high value so we don't do intermediate evals by default.\n    eval_every: int = 100000000,\n):\n    # this is per device!\n    if minibatch_size_per_device is None:\n        minibatch_size_per_device = 1\n    assert ds_name in VALID_DATASETS, f\"Unknown dataset {ds_name} not in {VALID_DATASETS}\"\n    if isinstance(transfer_loss, str):\n        transfer_losses = transfer_loss.split(\",\")\n    else:\n        transfer_losses = transfer_loss\n    del transfer_loss\n    for tloss in transfer_losses:\n        assert tloss in VALID_LOSSES, f\"Unknown loss {tloss} not in {VALID_LOSSES}\""
        },
        {
            "comment": "The code checks if the provided model sizes are valid, then fetches their configurations. It sets default learning rates and optimizers if not specified, and retrieves evaluation batch sizes from the models' configurations.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":166-194",
            "content": "    assert (\n        weak_model_size in MODELS_DICT\n    ), f\"Unknown model size {weak_model_size} not in {MODELS_DICT}\"\n    weak_model_config = MODELS_DICT[weak_model_size]\n    assert (\n        strong_model_size in MODELS_DICT\n    ), f\"Unknown model size {strong_model_size} not in {MODELS_DICT}\"\n    strong_model_config = MODELS_DICT[strong_model_size]\n    if weak_lr is None:\n        assert batch_size == 32\n        weak_lr = weak_model_config.default_lr\n    if strong_lr is None:\n        assert batch_size == 32\n        strong_lr = strong_model_config.default_lr\n    if transfer_lr is None:\n        transfer_lr = strong_lr\n    if transfer_epochs is None:\n        transfer_epochs = gt_epochs\n    if weak_optim is None:\n        weak_optim = weak_model_config.default_optimizer\n    if strong_optim is None:\n        strong_optim = strong_model_config.default_optimizer\n    if transfer_optim is None:\n        transfer_optim = strong_optim\n    weak_eval_batch_size = weak_model_config.eval_batch_size\n    strong_eval_batch_size = strong_model_config.eval_batch_size"
        },
        {
            "comment": "The code loads a dataset and splits the training data in half. It then defines a function `train_model` that takes a model configuration, training dataset, testing dataset, loss type, label, subpath, learning rate, evaluation batch size, number of epochs, optional inference dataset, and a boolean flag for linear probe. The function saves results in a specified folder with a suffix indicating if linear probe was used or not. It also configures the logger.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":196-224",
            "content": "    # Load dataset\n    dataset = load_dataset(ds_name, seed=seed, split_sizes=dict(train=n_docs, test=n_test_docs))\n    # Split the training dataset in half\n    train_dataset, test_ds = dataset[\"train\"], dataset[\"test\"]\n    split_data = train_dataset.train_test_split(test_size=0.5, seed=seed)\n    train1_ds, train2_ds = split_data[\"train\"], split_data[\"test\"]\n    print(\"len(train1):\", len(train1_ds), \"len(train2):\", len(train2_ds))\n    def train_model(\n        model_config: ModelConfig,\n        train_ds: torch.utils.data.Dataset,\n        test_ds: torch.utils.data.Dataset,\n        *,\n        loss_type: str,\n        label: str,\n        subpath,\n        lr,\n        eval_batch_size,\n        epochs=1,\n        inference_ds: Optional[torch.utils.data.Dataset] = None,\n        linear_probe: bool = False,\n        optimizer_name: str = \"adam\",\n    ):\n        save_path = os.path.join(results_folder, subpath)\n        linprobe_str = \"_linprobe\" if linear_probe else \"\"\n        logger.configure(\n            name=\"{log_prefix}{"
        },
        {
            "comment": "Creating an experiment name with specified parameters.\nTokenizing train, test, and optional inference datasets using the tokenizer for the given model.\nAssigning loss function based on specified loss type.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":224-248",
            "content": "label}_{base_model_name}_{ds_name}_{loss_type}_{optimizer_name}_{lr}_{lr_schedule}{linprobe_str}_{datetime_now}\",\n            label=label,\n            ds_name=ds_name,\n            truncation_max_len=n_docs or \"none\",\n            loss_type=loss_type,\n            lr=lr,\n            batch_size=batch_size,\n            eval_batch_size=eval_batch_size,\n            minibatch_size_per_device=minibatch_size_per_device,\n            save_path=save_path,\n            base_model_name=model_config.name,\n            epochs=epochs,\n            linprobe_str=linprobe_str,\n            lr_schedule=lr_schedule,\n            log_prefix=log_prefix,\n            optimizer_name=optimizer_name,\n        )\n        # Tokenize datasets\n        tokenizer = get_tokenizer(model_config.name)\n        train_ds = tokenize_dataset(train_ds, tokenizer, max_ctx)\n        test_ds = tokenize_dataset(test_ds, tokenizer, max_ctx)\n        if inference_ds:\n            inference_ds = tokenize_dataset(inference_ds, tokenizer, max_ctx)\n        loss_fn = loss_dict[loss_type]"
        },
        {
            "comment": "Trains a strong model using given parameters and returns the trained model.\n\nTrains a weak model on the first half of the training data with specified configuration.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":249-278",
            "content": "        return train_and_save_model(\n            model_config,\n            train_ds,\n            test_ds,\n            inference_ds=inference_ds,\n            batch_size=batch_size,\n            save_path=save_path,\n            loss_fn=loss_fn,\n            lr=lr,\n            epochs=epochs,\n            force_retrain=force_retrain,\n            eval_batch_size=eval_batch_size,\n            minibatch_size_per_device=minibatch_size_per_device,\n            train_with_dropout=train_with_dropout,\n            linear_probe=linear_probe,\n            lr_schedule=lr_schedule,\n            optimizer_name=optimizer_name,\n            eval_every=eval_every,\n        )\n    # Train the weak model on the first half of the training data\n    print(f\"Training weak model, size {weak_model_size}\")\n    weak_test_results, weak_ds = train_model(\n        weak_model_config,\n        train1_ds,\n        test_ds,\n        loss_type=\"xent\",\n        label=\"weak\",\n        subpath=os.path.join(\"weak_model_gt\", weak_model_size.replace(\"/\", \"_\")),\n        lr=weak_lr,"
        },
        {
            "comment": "Training the strong model on the second half of training data with varying loss functions.\nComment: This code trains a strong model using the second half of the training data, then trains another strong model using labels generated by the weak model while testing various transfer loss functions.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":279-306",
            "content": "        eval_batch_size=weak_eval_batch_size,\n        inference_ds=train2_ds,\n        epochs=gt_epochs,\n        linear_probe=linear_probe,\n        optimizer_name=weak_optim,\n    )\n    # Train the strong model on the second half of the training data\n    print(f\"Training strong model, size {strong_model_size}\")\n    strong_test_results, _ = train_model(\n        strong_model_config,\n        train2_ds,\n        test_ds,\n        loss_type=\"xent\",\n        label=\"strong\",\n        subpath=os.path.join(\"strong_model_gt\", strong_model_size.replace(\"/\", \"_\")),\n        lr=strong_lr,\n        eval_batch_size=strong_eval_batch_size,\n        epochs=gt_epochs,\n        linear_probe=linear_probe,\n        optimizer_name=strong_optim,\n    )\n    # Train the strong model on the second half of the training data with labels generated by the weak model\n    all_transfer_test_results = {}\n    for tloss in transfer_losses:\n        print(\n            f\"Training transfer model, size {strong_model_size} on labels from {weak_model_size}, with loss {tloss}\""
        },
        {
            "comment": "This code performs model transfer learning by training a strong model using the results of a weak model, and then evaluates the performance improvement. The results are stored in 'all_transfer_test_results' dictionary. It calculates the mean accuracy for both weak and strong models, and prints them out. Finally, it iterates over all transfer test results in the dictionary.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":307-335",
            "content": "        )\n        transfer_test_results, _ = train_model(\n            strong_model_config,\n            weak_ds,\n            test_ds,\n            loss_type=tloss,\n            label=\"weak2strong\",\n            subpath=os.path.join(\n                \"strong_model_transfer\",\n                f\"{weak_model_size.replace('/', '_')}_{strong_model_size.replace('/', '_')}_{tloss}\",\n            ),\n            lr=transfer_lr,\n            eval_batch_size=strong_eval_batch_size,\n            epochs=transfer_epochs,\n            linear_probe=linear_probe,\n            optimizer_name=transfer_optim,\n        )\n        all_transfer_test_results[tloss] = transfer_test_results\n        del transfer_test_results\n    weak_acc = np.mean([x[\"acc\"] for x in weak_test_results])\n    strong_acc = np.mean([x[\"acc\"] for x in strong_test_results])\n    res_dict = {\n        \"weak_acc\": weak_acc,\n        \"strong_acc\": strong_acc,\n    }\n    print(\"weak acc:\", weak_acc)\n    print(\"strong acc:\", strong_acc)\n    for tloss, transfer_test_results in all_transfer_test_results.items():"
        },
        {
            "comment": "Calculating transfer accuracy and storing results summary in a JSON file.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/train_weak_to_strong.py\":336-355",
            "content": "        transfer_acc = np.mean([x[\"acc\"] for x in transfer_test_results])\n        res_dict[f\"transfer_acc_{tloss}\"] = transfer_acc\n        print(f\"transfer acc ({tloss}):\", transfer_acc)\n    with open(\n        os.path.join(\n            results_folder,\n            f\"{weak_model_size.replace('/', '_')}_{strong_model_size.replace('/', '_')}.results_summary.json\",\n        ),\n        \"w\",\n    ) as f:\n        json.dump(\n            res_dict,\n            f,\n        )\n# python train_weak_to_strong.py --batch_size 32 --max_ctx 512 --ds_name \"sciq\" --transfer_loss \"logconf\" --n_docs 1000 --n_test_docs 100 --weak_model_size \"gpt2-medium\" --strong_model_size \"gpt2-large\" --seed 42\nif __name__ == \"__main__\":\n    fire.Fire(main)"
        }
    ]
}