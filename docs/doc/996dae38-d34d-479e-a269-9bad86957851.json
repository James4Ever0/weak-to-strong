{
    "summary": "The code trains and saves a model, handles sharded checkpoints, adjusts learning rates, uses data parallelism with multiple GPUs, checks accuracy, logs results, saves unwrapped models, and cleans up memory.",
    "details": [
        {
            "comment": "This code defines a `ModelConfig` class and a function `train_model` which takes in a model, dataset, batch size, learning rate, loss function, logging interval, evaluation interval, evaluation dataset (optional), and trains the model on the given dataset. The code also imports various libraries and classes from other modules.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":0-41",
            "content": "import itertools\nimport os\nimport pickle\nimport time\nfrom dataclasses import dataclass\nfrom typing import Callable, Optional\nimport datasets\nimport numpy as np\nimport torch\nimport torch_optimizer as toptim\nfrom transformers.modeling_utils import load_sharded_checkpoint\nimport weak_to_strong.logger as logger\nfrom weak_to_strong.common import clear_mem\nfrom weak_to_strong.eval import eval_model_acc\nfrom weak_to_strong.loss import xent_loss\nfrom weak_to_strong.model import TransformerWithHead\n@dataclass\nclass ModelConfig:\n    name: str\n    default_lr: float\n    eval_batch_size: int\n    custom_kwargs: Optional[dict] = None\n    gradient_checkpointing: bool = False\n    model_parallel: bool = False\n    default_optimizer: str = \"adam\"\ndef train_model(\n    model: torch.nn.Module,\n    ds: datasets.Dataset,\n    batch_size: int,\n    lr: float = 1e-5,\n    loss_fn: Callable = xent_loss,\n    log_every: int = 10,\n    eval_every: int = 100,\n    eval_batch_size: int = 256,\n    minibatch_size: int = 8,\n    eval_ds: Optional[datasets.Dataset] = None,"
        },
        {
            "comment": "This code is initializing a model for training, setting the learning rate (lr), batch size, and minibatch size. It checks if the batch size is divisible by the minibatch size. If the train_with_dropout flag is True, it turns on dropout in the model, otherwise, it sets the model to evaluation mode. If gradient_checkpointing is enabled, it enables gradient checkpointing. It calculates the number of steps (nsteps) based on the dataset length and number of epochs. Finally, it initializes an optimizer (Adam) with the specified learning rate.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":42-70",
            "content": "    gradient_checkpointing: bool = False,\n    train_with_dropout: bool = False,\n    epochs: int = 1,\n    lr_schedule: str = \"cosine_anneal\",\n    optimizer_name: str = \"adam\",\n):\n    print(\"LR\", lr, \"batch_size\", batch_size, \"minibatch_size\", minibatch_size)\n    assert batch_size % minibatch_size == 0, \"batch size must be divisible by minibatch size\"\n    # we purposefully turn off dropout, for determinism\n    # this seems to help for 1 epoch finetuning anyways\n    if train_with_dropout:\n        model.train()\n    else:\n        model.eval()\n    if gradient_checkpointing:\n        (\n            model if hasattr(model, \"gradient_checkpointing_enable\") else model.module\n        ).gradient_checkpointing_enable()\n    nsteps = len(ds) * epochs // batch_size\n    def lr_schedule_fn(step):\n        if lr_schedule == \"constant\":\n            return 1\n        else:\n            assert False, f\"invalid lr schedule, {lr_schedule}, must be constant or cosine_anneal\"\n    if optimizer_name.lower() == \"adam\":\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
        },
        {
            "comment": "This code checks the optimizer name and initializes either Adafactor or Adam optimizer accordingly. It then sets up the learning rate scheduler based on the given lr_schedule. The code keeps track of loss and accuracy values throughout training and evaluates the model periodically. If the model is wrapped by DataParallel, it uses GPU 0 as the output device.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":71-93",
            "content": "    elif optimizer_name.lower() == \"adafactor\":\n        optimizer = toptim.Adafactor(model.parameters(), lr=lr)\n    else:\n        assert False, f\"invalid optimizer {optimizer_name}, must be adam or adafactor\"\n    if lr_schedule == \"cosine_anneal\":\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, nsteps)\n    else:\n        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule_fn)\n    step = 0\n    it = itertools.chain.from_iterable(itertools.repeat(ds, epochs))\n    losses = []\n    accuracies = []\n    eval_acc_dict = {}\n    # If the model is wrapped by DataParallel, it doesn't have a device. In this case,\n    # we use GPU 0 as the output device. This sadly means that this device will store\n    # a bit more data than other ones, but hopefully should not be too big of a deal.\n    io_device = model.device if hasattr(model, \"device\") else 0\n    while step < nsteps:\n        loss_tot = 0\n        if eval_every and step % eval_every == 0:\n            eval_results = eval_model_acc(model, eval_ds, eval_batch_size)"
        },
        {
            "comment": "Checks if gradient checkpointing is enabled and enables it if true.\nEnables dropout training for the model.\nCalculates the mean accuracy from evaluation results.\nLogs the evaluation accuracy in the logger.\nInitializes empty lists for storing all logits and labels.\nIterates through mini-batches, padding input_ids, and collecting logits and labels.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":94-120",
            "content": "            if gradient_checkpointing:\n                (\n                    model if hasattr(model, \"gradient_checkpointing_enable\") else model.module\n                ).gradient_checkpointing_enable()\n            if train_with_dropout:\n                model.train()\n            eval_accs = np.mean([r[\"acc\"] for r in eval_results])\n            eval_acc_dict[step] = eval_accs\n            logger.logkv(\"eval_accuracy\", eval_accs)\n        all_logits = []\n        all_labels = []\n        for i in range(batch_size // minibatch_size):\n            try:\n                mbatch = [next(it) for _ in range(minibatch_size)]\n            except StopIteration:\n                break\n            input_ids = (\n                torch.nn.utils.rnn.pad_sequence([torch.tensor(ex[\"input_ids\"]) for ex in mbatch])\n                .transpose(\n                    0,\n                    1,\n                )\n                .to(io_device)\n            )\n            labels = torch.tensor([ex[\"soft_label\"] for ex in mbatch]).to(io_device)\n            logits = model(input_ids)"
        },
        {
            "comment": "This code is training a model by calculating the loss and accuracy, logging progress, updating the optimizer, and scheduling learning rate adjustments.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":122-151",
            "content": "            all_logits.extend(logits.to(io_device))\n            all_labels.extend(labels)\n        all_logits = torch.stack(all_logits)\n        all_labels = torch.stack(all_labels)\n        loss = loss_fn(all_logits, all_labels, step_frac=step / nsteps)\n        loss_tot += loss.item()\n        loss.backward()\n        losses.append(loss_tot)\n        accuracies.append(\n            torch.mean(\n                (torch.argmax(all_logits, dim=1) == torch.argmax(all_labels, dim=1)).to(\n                    torch.float32\n                )\n            ).item()\n        )\n        logger.logkvs(\n            {\n                \"step\": step,\n                \"progress\": step / nsteps,\n                \"loss\": loss_tot,\n                \"train_accuracy\": accuracies[-1],\n                \"lr\": lr_scheduler.get_last_lr()[0],\n            }\n        )\n        optimizer.step()\n        optimizer.zero_grad()\n        lr_scheduler.step()\n        if log_every and step % log_every == 0:\n            print(\n                f\"Step: {step}/{nsteps} Recent losses: {np.mean(losses)} {np.mean(accuracies)} {len(losses)}\""
        },
        {
            "comment": "Training and saving a model with specified configuration, training dataset, test dataset, optional inference dataset, batch size, learning rate, number of epochs, evaluation batch size (optional), mini-batch size per device (optional), save path (optional), loss function, label (default), force retraining (bool), train with dropout (bool), linear probe (bool), learning rate schedule (str), optimizer name (str), and evaluation interval (optional).",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":152-185",
            "content": "            )\n            losses = []\n            accuracies = []\n        step += 1\n        logger.dumpkvs()\n    final_eval_results = None\n    if eval_every:\n        print(\"Final evaluation:\")\n        final_eval_results = eval_model_acc(model, eval_ds, eval_batch_size)\n        logger.logkv(\"eval_accuracy\", np.mean([r[\"acc\"] for r in final_eval_results]))\n        logger.dumpkvs()\n    return final_eval_results\ndef train_and_save_model(\n    model_config: ModelConfig,\n    train_ds: datasets.Dataset,\n    test_ds: datasets.Dataset,\n    inference_ds: Optional[datasets.Dataset] = None,\n    *,\n    batch_size: int,\n    lr: float,\n    epochs: int,\n    eval_batch_size: Optional[int] = None,\n    minibatch_size_per_device: Optional[int] = None,\n    save_path: Optional[str] = None,\n    loss_fn: Callable = xent_loss,\n    label: str = \"default\",\n    force_retrain: bool = False,\n    train_with_dropout: bool = False,\n    linear_probe: bool = False,\n    lr_schedule: str = \"constant\",\n    optimizer_name: str = \"adam\",\n    eval_every: Optional[int] = None,"
        },
        {
            "comment": "This code checks if the model exists and loads it from the save path if not forcing retraining. It also handles sharded checkpoints by loading them appropriately and updates the state_dict to match the correct module names.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":186-209",
            "content": "):\n    if eval_batch_size is None:\n        eval_batch_size = batch_size\n    if minibatch_size_per_device is None:\n        minibatch_size_per_device = 1\n    gradient_checkpointing = model_config.gradient_checkpointing\n    custom_kwargs = model_config.custom_kwargs or {}\n    def maybe_load_model(model):\n        if os.path.exists(os.path.join(save_path, \"results.pkl\")) and not force_retrain:\n            print(\"loading from\", save_path)\n            checkpoint_path = os.path.join(save_path, \"pytorch_model.bin\")\n            if not os.path.exists(checkpoint_path):\n                # Assume this means we have a sharded checkpoint, and load it appropriately\n                load_sharded_checkpoint(model, checkpoint_path)\n            else:\n                state_dict = torch.load(os.path.join(save_path, \"pytorch_model.bin\"))\n                state_dict = {\n                    k.replace(\"transformer.module\", \"transformer\"): v\n                    for (k, v) in state_dict.items()\n                }\n                custom_kwargs[\"state_dict\"] = state_dict"
        },
        {
            "comment": "This code is setting up the model configuration and loading the model. It checks if the device count is greater than 1, creates a TransformerWithHead model with two labels and the specified custom arguments. It then checks if the model has already been trained and sets the mini-batch size accordingly based on the device count. If there are multiple GPUs, it uses data parallelism by wrapping the model in torch.nn.DataParallel.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":210-235",
            "content": "            return True\n        return False\n    already_trained = False\n    # Load the model\n    if model_config.model_parallel:\n        assert torch.cuda.device_count() > 1, f\"you might want more gpus for {model_config.name}\"\n        model = TransformerWithHead.from_pretrained(\n            model_config.name,\n            num_labels=2,\n            device_map=\"auto\",\n            linear_probe=linear_probe,\n            **custom_kwargs,\n        )\n        already_trained = maybe_load_model(model)\n        # slight misnomer, more like minibatch_size_per_dp_replica\n        minibatch_size = minibatch_size_per_device\n    else:\n        model = TransformerWithHead.from_pretrained(\n            model_config.name, num_labels=2, linear_probe=linear_probe, **custom_kwargs\n        ).to(\"cuda\")\n        already_trained = maybe_load_model(model)\n        # data parallel:  currently not supported with model parallel\n        if torch.cuda.device_count() > 1:\n            model = torch.nn.DataParallel(model, output_device=0)\n            minibatch_size = min(minibatch_size_per_device * torch.cuda.device_count(), batch_size)"
        },
        {
            "comment": "The code is checking if the model has already been trained. If it has, it evaluates the model's accuracy on the test dataset (test_results). If not, it trains the model using the specified parameters and evaluates its performance on the test dataset after training is complete (train_model function). The time taken for model training is also printed. Additionally, if a save path is provided, the code unwraps the model before saving it (if wrapped by DataParallel).",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":236-265",
            "content": "            print(\n                \"Using\",\n                torch.cuda.device_count(),\n                \"GPUs, setting minibatch_size to\",\n                minibatch_size,\n            )\n    if already_trained:\n        test_results = eval_model_acc(model, test_ds, eval_batch_size)\n    else:\n        start = time.time()\n        test_results = train_model(\n            model,\n            train_ds,\n            batch_size,\n            lr=lr,\n            epochs=epochs,\n            eval_ds=test_ds,\n            gradient_checkpointing=gradient_checkpointing,\n            loss_fn=loss_fn,\n            eval_batch_size=eval_batch_size,\n            eval_every=eval_every,\n            minibatch_size=minibatch_size,\n            train_with_dropout=train_with_dropout,\n            lr_schedule=lr_schedule,\n            optimizer_name=optimizer_name,\n        )\n        print(\"Model training took\", time.time() - start, \"seconds\")\n        if save_path:\n            # Note: If the model is wrapped by DataParallel, we need to unwrap it before saving"
        },
        {
            "comment": "Saving the model and results to disk, logging inference accuracy, and cleaning up memory",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":266-291",
            "content": "            (model if hasattr(model, \"save_pretrained\") else model.module).save_pretrained(\n                save_path\n            )\n            print(\"saved\", save_path)\n    inference_results = None\n    if inference_ds:\n        inference_results = eval_model_acc(model, inference_ds, eval_batch_size)\n        logger.logkv(\"inference_accuracy\", np.mean([r[\"acc\"] for r in inference_results]))\n    if save_path:\n        with open(os.path.join(save_path, \"results.pkl\"), \"wb\") as f:\n            pickle.dump(\n                {\n                    \"avg_acc_test\": float(np.mean([r[\"acc\"] for r in test_results])),\n                    \"avg_acc_inference\": float(\n                        np.mean([r[\"acc\"] for r in inference_results] if inference_results else [])\n                    ),\n                    \"test_results\": test_results,\n                    \"inference_results\": inference_results if inference_results else [],\n                },\n                f,\n            )\n    # try to clean up memory\n    clear_mem()\n    logger.shutdown()"
        },
        {
            "comment": "Returns test and inference results.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/train.py\":293-293",
            "content": "    return test_results, inference_results"
        }
    ]
}