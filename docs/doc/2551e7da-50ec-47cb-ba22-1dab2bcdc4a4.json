{
    "summary": "The code defines a function eval_model_acc that evaluates model accuracy on a dataset, iterating through batches in evaluation mode without gradient descent. It returns various data and calculates the accuracy with standard deviation before returning a dataset from the results.",
    "details": [
        {
            "comment": "This code defines a function eval_model_acc which takes a model, a dataset and an optional evaluation batch size, and returns a list of dictionaries containing the input ids, ground truth labels, predicted labels, accuracy of predictions, logits and soft labels for each example in the dataset. It evaluates the given model's accuracy on the given dataset by iterating through batches of examples from the dataset while keeping the model in evaluation mode and without performing gradient descent.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/eval.py\":0-35",
            "content": "import datasets\nimport numpy as np\nimport torch\nfrom torch import nn\ndef to_batch(x, batch_size):\n    for i in range(0, len(x), batch_size):\n        yield x[i : i + batch_size]\ndef unpack(x):\n    assert isinstance(x, torch.Tensor), type(x)\n    return x.detach().float().cpu().numpy().tolist()\ndef eval_model_acc(model: nn.Module, ds: datasets.Dataset, eval_batch_size: int = 16) -> None:\n    \"\"\"\n    This function evaluates the accuracy of a given model on a given dataset.\n    Parameters:\n    model (nn.Module): The model to be evaluated.\n    ds (datasets.Dataset): The dataset on which the model is to be evaluated.\n    Returns:\n    results (list): A list of dictionaries containing the input_ids, ground truth label, predicted label,\n                    accuracy of prediction, logits and soft label for each example in the dataset.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        results = []\n        # for ex in ds:\n        for batch in to_batch(ds, eval_batch_size):\n            # pad input_ids to common length"
        },
        {
            "comment": "This code prepares input_ids, pads sequences, and runs a forward pass on the model to get logits. It then applies softmax function to get probabilities and argmax to get predictions and ground truth labels. Finally, it creates a list of results with text, input IDs, ground truth labels, predicted labels, accuracy, logits, and softmax probabilities.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/eval.py\":36-61",
            "content": "            input_ids = torch.nn.utils.rnn.pad_sequence(\n                [torch.tensor(ex) for ex in batch[\"input_ids\"]], batch_first=True\n            ).to(model.device if hasattr(model, \"device\") else \"cpu\")\n            labels = batch[\"soft_label\"]\n            # run forward pass\n            raw_logits = model(input_ids)\n            probs = unpack(torch.nn.functional.softmax(raw_logits, dim=-1))\n            logits = unpack(raw_logits)\n            preds = np.argmax(probs, axis=-1)\n            labels = np.argmax(labels, axis=-1)\n            results.extend(\n                [\n                    dict(\n                        txt=txt,\n                        input_ids=input_id,\n                        gt_label=label,\n                        hard_label=pred,\n                        acc=label == pred,\n                        logits=logit,\n                        soft_label=prob,\n                    )\n                    for input_id, txt, label, pred, prob, logit in zip(\n                        batch[\"input_ids\"], batch[\"txt\"], labels, preds, probs, logits"
        },
        {
            "comment": "Calculates accuracy and prints it with standard deviation from the results list. Then returns a dataset from the results.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/eval.py\":62-68",
            "content": "                    )\n                ]\n            )\n        accs = [r[\"acc\"] for r in results]\n        print(\"Accuracy:\", np.mean(accs), \"+/-\", np.std(accs) / np.sqrt(len(accs)))\n        return datasets.Dataset.from_list(results)"
        }
    ]
}