{
    "summary": "The code defines a `DatasetConfig` class for dataset loading, splitting, and tokenization. It includes functions to load new datasets with their respective loaders and formatters, and initializes two configurations: \"cosmos_qa\" and \"boolq\". It loads the specified datasets, prints information about them, and calculates mean hard label for training examples.",
    "details": [
        {
            "comment": "This code defines a class `DatasetConfig` and registers datasets with their corresponding loaders and formatters. The function `load_dataset()` loads datasets by name, split, and optional number of documents per split.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/datasets.py\":0-34",
            "content": "import functools\nfrom dataclasses import dataclass\nfrom random import Random\nfrom typing import Any, Callable, Optional\nfrom datasets import Dataset as HfDataset\nfrom datasets import load_dataset as hf_load_dataset\n@dataclass\nclass DatasetConfig:\n    # split -> unshuffled dataset of items\n    loader: Callable[[str], HfDataset]\n    # formats items to have keys 'txt' and 'hard_label', takes a random.Random rng\n    formatter: Callable[[Any], Any]\n# mapping from dataset name to load function and format function\n_REGISTRY: dict[str, DatasetConfig] = {}\ndef register_dataset(name: str, config: DatasetConfig):\n    _REGISTRY[name] = config\ndef load_dataset(ds_name: str, seed: int = 0, split_sizes: Optional[dict] = None):\n    if split_sizes is None:\n        split_sizes = dict(train=None, test=None)\n    if ds_name not in _REGISTRY:\n        raise ValueError(f\"Unknown dataset {ds_name}, please register\")\n    cfg = _REGISTRY[ds_name]\n    results = {}\n    for split, n_docs in split_sizes.items():\n        ds = cfg.loader(split)"
        },
        {
            "comment": "This code is preparing a dataset for training. It selects the desired number of documents, applies formatting and tokenization, and shuffles the data.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/datasets.py\":35-66",
            "content": "        try:\n            ds = ds.select(range(n_docs))\n        except IndexError as e:\n            print(f\"Warning {ds_name} has less than {n_docs} docs, using all: {e}\")\n        ds = ds.map(functools.partial(cfg.formatter, rng=Random(seed)))\n        ds = ds.map(\n            lambda ex: {\"soft_label\": [1 - float(ex[\"hard_label\"]), float(ex[\"hard_label\"])]}\n        )\n        ds = ds.shuffle(seed=seed)  # shuffling a bit pointless for test set but wtv\n        results[split] = ds\n    return results\ndef tokenize_dataset(\n    raw_ds: HfDataset,\n    tokenizer: Callable,\n    max_ctx: int,\n):\n    \"\"\"\n    This function prepares the dataset for training. It takes the raw dataset, a formatting function,\n    a tokenizer, a maximum context length\n    Parameters:\n    raw_ds: The raw dataset to be processed.\n    tokenizer: The tokenizer to be used on the formatted dataset.\n    max_ctx: The maximum context length for the tokenizer.\n    Returns:\n    ds: The processed and shuffled dataset ready for training.\n    \"\"\"\n    def process_function(res):"
        },
        {
            "comment": "This code defines a function `process_function` that takes input text, tokenizes it using the `tokenizer`, and returns the \"input_ids\" as a dictionary. The `hf_loader` function is used to load datasets with different names (specified by hf_name) and splits (specified by split_names).\n\nThe code then defines two functions, `format_amazon_polarity` and `format_sciq`, which format the data for the respective datasets. These formatted datasets are registered using the `register_dataset` function.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/datasets.py\":67-108",
            "content": "        toks = tokenizer(res[\"txt\"])\n        return dict(\n            input_ids=toks[\"input_ids\"],\n        )\n    ds = raw_ds.map(process_function, batched=False).filter(lambda x: len(x[\"input_ids\"]) < max_ctx)\n    return ds\ndef hf_loader(*hf_name, split_names=None):\n    if split_names is None:\n        split_names = dict()\n    return lambda split: hf_load_dataset(*hf_name, split=split_names.get(split, split))\n##########\n# ACTUAL DATASETS\n##########\ndef format_amazon_polarity(ex, rng):\n    return dict(txt=f\"{ex['title']} {ex['content']}\", hard_label=ex[\"label\"])\nregister_dataset(\n    \"amazon_polarity\",\n    DatasetConfig(loader=hf_loader(\"amazon_polarity\"), formatter=format_amazon_polarity),\n)\ndef format_sciq(ex, rng):\n    hard_label = int(rng.random() < 0.5)\n    if hard_label:\n        ans = ex[\"correct_answer\"]\n    else:\n        ans = rng.choice([ex[\"distractor1\"], ex[\"distractor2\"], ex[\"distractor3\"]])\n    txt = f\"Q: {ex['question']} A: {ans}\"\n    return dict(txt=txt, hard_label=hard_label)\nregister_dataset(\n    \"sciq\","
        },
        {
            "comment": "This code defines two datasets, \"anthropic_hh\" and \"cosmos_qa\", with their respective loaders and formatters. The \"format_anthropic_hh\" function assigns a hard label to examples randomly, while the \"format_cosmosqa\" function generates a formatted text based on whether the true answer is one of the multiple-choice options or not.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/datasets.py\":109-142",
            "content": "    DatasetConfig(loader=hf_loader(\"sciq\"), formatter=format_sciq),\n)\ndef format_anthropic_hh(ex, rng):\n    hard_label = int(rng.random() < 0.5)\n    txt = ex[\"chosen\"] if hard_label else ex[\"rejected\"]\n    return dict(txt=txt, hard_label=hard_label)\nregister_dataset(\n    \"anthropic_hh\",\n    DatasetConfig(loader=hf_loader(\"Anthropic/hh-rlhf\"), formatter=format_anthropic_hh),\n)\ndef format_cosmosqa(ex, rng):\n    true_answer = ex[\"answer\" + str(ex[\"label\"])]\n    if \"None of the above choices .\" in true_answer:\n        hard_label = 0\n    else:\n        assert \"None of the above choices\" not in true_answer, true_answer\n        hard_label = int(rng.random() < 0.5)\n    if hard_label:\n        answer = true_answer\n    else:\n        candidate_answers = [ex[\"answer\" + str(i)] for i in range(4)]\n        answer = rng.choice([x for x in candidate_answers if x != true_answer])\n    txt = f\"Context: {ex['context']}\\nQuestion: {ex['question']}\\nAnswer: {answer}\"\n    return dict(txt=txt, hard_label=hard_label)\nregister_dataset(\n    \"cosmos_qa\","
        },
        {
            "comment": "The code imports necessary libraries and defines two dataset configurations: \"cosmos_qa\" and \"boolq\". It then initializes a list of valid datasets, loads the specified datasets, prints some information about the loaded data (including one example from the test split), and calculates the mean hard label for the training examples.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/datasets.py\":143-181",
            "content": "    DatasetConfig(\n        loader=hf_loader(\"cosmos_qa\", split_names=dict(test=\"validation\")),\n        formatter=format_cosmosqa,\n    ),\n)\ndef format_boolq(ex, rng):\n    hard_label = int(ex[\"answer\"])\n    txt = f\"Passage: {ex['passage']}\\nQuestion: {ex['question']}\"\n    return dict(txt=txt, hard_label=hard_label)\nregister_dataset(\n    \"boolq\",\n    DatasetConfig(\n        loader=hf_loader(\"boolq\", split_names=dict(test=\"validation\")), formatter=format_boolq\n    ),\n)\nVALID_DATASETS: list[str] = list(_REGISTRY.keys())\n\"\"\"\nfrom datasets import disable_caching\ndisable_caching()\nfrom weak_to_strong.datasets import load_dataset, VALID_DATASETS\nimport numpy as np\nds_name = \"boolq\"\nprint(VALID_DATASETS)\nds = load_dataset(ds_name, split_sizes=dict(train=500, test=10))\ntrain = list(ds['train'])\ntest = list(ds['test'])\nprint(test[0])\nprint(np.mean([x['hard_label'] for x in train]))\n\"\"\""
        }
    ]
}