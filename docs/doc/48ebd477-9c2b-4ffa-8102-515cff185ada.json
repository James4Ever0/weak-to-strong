{
    "summary": "The code defines a base class for loss functions, including xent_loss (cross entropy loss), product_loss_fn, and a custom loss function \"logconf_loss_fn\" that calculates cross entropy loss using logits and labels, applying power functions based on beta and alpha values with an optional warmup_frac parameter.",
    "details": [
        {
            "comment": "The code defines a base class for loss functions and two custom loss functions, xent_loss (cross entropy loss) and product_loss_fn (custom loss function). The custom loss functions extend the LossFnBase class and override the __call__ method to calculate their respective losses.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/loss.py\":0-41",
            "content": "import torch\nclass LossFnBase:\n    def __call__(\n        self,\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        **kwargs,\n    ) -> torch.Tensor:\n        \"\"\"\n        This function calculates the loss between logits and labels.\n        \"\"\"\n        raise NotImplementedError\n# Custom loss function\nclass xent_loss(LossFnBase):\n    def __call__(\n        self, logits: torch.Tensor, labels: torch.Tensor, step_frac: float\n    ) -> torch.Tensor:\n        \"\"\"\n        This function calculates the cross entropy loss between logits and labels.\n        Parameters:\n        logits: The predicted values.\n        labels: The actual values.\n        step_frac: The fraction of total training steps completed.\n        Returns:\n        The mean of the cross entropy loss.\n        \"\"\"\n        loss = torch.nn.functional.cross_entropy(logits, labels)\n        return loss.mean()\nclass product_loss_fn(LossFnBase):\n    \"\"\"\n    This class defines a custom loss function for product of predictions and labels.\n    Attributes:\n    alpha: A float indicating how much to weigh the weak model."
        },
        {
            "comment": "This code defines a custom loss function for log confidence by creating a class named \"logconf_loss_fn\". The function takes in logits and labels as input, calculates the target using softmax, applies a power function based on the defined beta value to the predictions, another power function based on the alpha value to the labels, normalizes the target, and finally calculates the cross entropy loss. This custom loss is then applied for training purposes. The class also includes an optional warmup_frac parameter that can be used during initial training steps.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/loss.py\":42-72",
            "content": "    beta: A float indicating how much to weigh the strong model.\n    warmup_frac: A float indicating the fraction of total training steps for warmup.\n    \"\"\"\n    def __init__(\n        self,\n        alpha: float = 1.0,  # how much to weigh the weak model\n        beta: float = 1.0,  # how much to weigh the strong model\n        warmup_frac: float = 0.1,  # in terms of fraction of total training steps\n    ):\n        self.alpha = alpha\n        self.beta = beta\n        self.warmup_frac = warmup_frac\n    def __call__(\n        self,\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        step_frac: float,\n    ) -> torch.Tensor:\n        preds = torch.softmax(logits, dim=-1)\n        target = torch.pow(preds, self.beta) * torch.pow(labels, self.alpha)\n        target /= target.sum(dim=-1, keepdim=True)\n        target = target.detach()\n        loss = torch.nn.functional.cross_entropy(logits, target, reduction=\"none\")\n        return loss.mean()\nclass logconf_loss_fn(LossFnBase):\n    \"\"\"\n    This class defines a custom loss function for log confidence."
        },
        {
            "comment": "This code defines a class for a loss function that takes in logits and labels, applies a softmax function to the logits, calculates a threshold based on the weak labels mean, and then performs classification by comparing the strong label to the calculated threshold. The auxiliary coefficient and warmup fraction can be set during initialization.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/loss.py\":74-103",
            "content": "    Attributes:\n    aux_coef: A float indicating the auxiliary coefficient.\n    warmup_frac: A float indicating the fraction of total training steps for warmup.\n    \"\"\"\n    def __init__(\n        self,\n        aux_coef: float = 0.5,\n        warmup_frac: float = 0.1,  # in terms of fraction of total training steps\n    ):\n        self.aux_coef = aux_coef\n        self.warmup_frac = warmup_frac\n    def __call__(\n        self,\n        logits: torch.Tensor,\n        labels: torch.Tensor,\n        step_frac: float,\n    ) -> torch.Tensor:\n        logits = logits.float()\n        labels = labels.float()\n        coef = 1.0 if step_frac > self.warmup_frac else step_frac\n        coef = coef * self.aux_coef\n        preds = torch.softmax(logits, dim=-1)\n        mean_weak = torch.mean(labels, dim=0)\n        assert mean_weak.shape == (2,)\n        threshold = torch.quantile(preds[:, 0], mean_weak[1])\n        strong_preds = torch.cat(\n            [(preds[:, 0] >= threshold)[:, None], (preds[:, 0] < threshold)[:, None]],\n            dim=1,"
        },
        {
            "comment": "This code calculates a weighted average of true labels and strong predictions, then applies cross entropy loss to logits and these target values. The result is the mean loss.",
            "location": "\"/media/root/Toshiba XG3/works/weak-to-strong/docs/src/weak_to_strong/loss.py\":104-107",
            "content": "        )\n        target = labels * (1 - coef) + strong_preds.detach() * coef\n        loss = torch.nn.functional.cross_entropy(logits, target, reduction=\"none\")\n        return loss.mean()"
        }
    ]
}